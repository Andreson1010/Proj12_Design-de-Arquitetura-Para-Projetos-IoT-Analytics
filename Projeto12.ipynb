{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f78c66-6753-44a9-ad63-803a479dc9ae",
   "metadata": {},
   "source": [
    "# Spark Streaming com Kafka para Geração de Streaming\n",
    "\n",
    "Configurando o Spark Streaming para ler dados da fonte e publicar no Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a167d5-4f1f-4ce0-8a49-4d3784d5c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233f9a32-14dc-4f72-ac25-9cf96db9f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593646ec-cb94-45ad-a735-b536ef391318",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f489b5fd-eed9-4868-9c94-ca10cbea0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SmartPipeNet\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf28a5-23eb-4a21-9938-f490c9523fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo dados da fonte \n",
    "\n",
    "# Cada tópico é lido em um DataFrame separado e, em seguida, cada um desses DataFrames é \n",
    "# escrito de volta para o tópico correspondente. \n",
    "from pyspark.sql.functions import to_json, struct\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializando a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"SmartPipeNet\").getOrCreate()\n",
    "\n",
    "def write_to_kafka_topic(df, topic, checkpoint_dir):\n",
    "    # Convertendo todas as colunas do DataFrame em uma única coluna de string JSON\n",
    "    json_df = df.select(to_json(struct(*[df[col] for col in df.columns])).alias(\"value\"))\n",
    "\n",
    "    json_df.writeStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"topic\", topic) \\\n",
    "        .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "        .start()\n",
    "\n",
    "# Definindo os DataFrames de streaming\n",
    "df_leak_detection = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"leak-detection\") \\\n",
    "    .load()\n",
    "\n",
    "df_predictive_maintenance = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"predictive-maintenance\") \\\n",
    "    .load()\n",
    "\n",
    "df_flow_regulation = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"flow-regulation\") \\\n",
    "  .load()\n",
    "\n",
    "df_system_control = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"system-control\") \\\n",
    "  .load()\n",
    "\n",
    "# Escrevendo para os tópicos Kafka com localizações de checkpoint especificadas\n",
    "write_to_kafka_topic(df_leak_detection, \"leak-detection\", \"C:\\\\Projeto12\\checkpoint\\leak-detection\")\n",
    "write_to_kafka_topic(df_predictive_maintenance, \"predictive-maintenance\", \"C:\\\\Projeto12\\checkpoint\\predictive-maintenance\")\n",
    "write_to_kafka_topic(df_flow_regulation, \"flow-regulation\", \"C:\\\\Projeto12\\checkpoint\\ flow-regulation\")\n",
    "write_to_kafka_topic(df_system_control, \"system-control\", \"C:\\\\Projeto12\\checkpoint\\system-control\")\n",
    "\n",
    "spark.streams.awaitAnyTermination()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7d438-531c-4a2a-9c6f-80b50baf5802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
